{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Abhishek Bhatia, Tomas McIntee, John Powers  \n",
    "Â© 2025, The University of North Carolina at Chapel Hill. Permission is granted to use in accordance with the MIT license.  \n",
    "The code is licensed under the MIT license.  \n",
    "\n",
    "\n",
    "***Model Training Process***\n",
    "\n",
    "For the training process, three data source tables were combined with two OMOP concept tables and a variable rollup table borrowed from a previously published model.\n",
    "\n",
    "**full_cohort**, the dataset generated by the Diagnosis-cohort-building pipeline.\n",
    "* *person_id*, an identifier used to link persons.\n",
    "* *index_date*, assigned as earliest date of diagnosis with ME/CFS or PASC for known positives and otherwise assigned randomly within a patient's temporal record.\n",
    "* *window_start*, assigned in this training process as 30 days prior to *index_date*.\n",
    "* *window_end*, assigned in this training process as 30 days prior to *index_date*\n",
    "* *cohort_label*, coded as PASC, CFS, PASC_CFS, or CTRL.\n",
    "* *pre_covid*, an indicator of whether or not the index date is before the COVID-19 era.\n",
    "* *PASC_index*, date of PASC diagnosis, usually null.\n",
    "* *CFS_index*, date of CFS diagnosis, usually null.\n",
    "* *is_female* Used for matching and as a model variable.\n",
    "* *age_bin* Used for matching and as a model variable.\n",
    "* *ethnicity* Used for matching only.\n",
    "* *race* Used for matching only.\n",
    "* *cci_score* Used for matching only.\n",
    "\n",
    "**infections_cohort**, a dataset of infections containing the following columns:\n",
    "* *person_id*, an identifier used to link persons.\n",
    "* *infection_date*, date of a COVID-19 infection. In our data, based on a minimum date between diagnosis and prescriptions for either Paxlovid or Remdesivir.\n",
    "* *infection_count*, a within-person index of how many prior infections the person has had.\n",
    "* *blackout_begin*, set in this work at 7 days prior to infection index.\n",
    "* *blackout_end*, set in this work at 28 days prior to infection index.\n",
    "\n",
    "**condition_occurrence**, an OMOP-formatted table containing the following important columns:\n",
    "* *person_id*, an identifier used to link persons.\n",
    "* *condition_start_date*, used \n",
    "* *condition_concept_id*, a unique identifier for a condition\n",
    "* *condition_concept_name*, a human-readable condition name useful for model evaluation and interpretation.\n",
    "\n",
    "**rollup_lookup**, a table designating which rarer conditions should be combined. This was generated based on statistically desirable minimum counts and was used to simplify the **condition_occurrence** table. The following four data columns were utilized:\n",
    "* *ancestor_concept_id* The ID number of a common condition.\n",
    "* *descendant_concept_id* The ID number of a rare condition to be rolled up.\n",
    "* *anc_concept_name* The name of the common condition.\n",
    "* *des_concept_name* The name of the rare condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "from foundry_ml import Model, Stage\n",
    "from pandas import DataFrame\n",
    "import shap\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "import pandas\n",
    "import numpy\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# R requirements\n",
    "require(MatchIt)\n",
    "require(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cohort generation***\n",
    "Matched training cohorts were generated with the *MatchIt* package. This took place within R, although alternative matching techniques are available in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Set seed and training fraction for replicability\n",
    "set.seed(37)\n",
    "training_fraction <- 0.8\n",
    "# Original Python code included for reference. R equivalent code provided for convenience.\n",
    "\n",
    "# seed_choice = 37 # Replicability, but note that Spark poses issues.\n",
    "# For patients who are diagnosed with both PASC and CFS, index date needs to be corrected as necessary.\n",
    "# CFS_cohort = full_cohort.filter(\"cohort_label == 'CFS' or cohort_label == 'PASC'\") \\\n",
    "#    .withColumn(\"window_start\",full_cohort.window_start + (full_cohort.CFS_index - full_cohort.index_date)) \\\n",
    "#    .withColumn(\"window_end\",full_cohort.window_end + (full_cohort.CFS_index - full_cohort.index_date)) \\\n",
    "#    .withColumn(\"pre_covid\",full_cohort.pre_covid)\n",
    "# PASC_cohort = full_cohort.filter(\"cohort_label == 'PASC' or cohort_label == 'PASC'\") \\\n",
    "#    .withColumn(\"window_start\",full_cohort.window_start + (full_cohort.PASC_index - full_cohort.index_date)) \\\n",
    "#    .withColumn(\"window_end\",full_cohort.window_end + (full_cohort.PASC_index - full_cohort.index_date)) \\\n",
    "#    .withColumn(\"pre_covid\",full_cohort.pre_covid_PASC)\n",
    "#\n",
    "# Base cohorts:\n",
    "CTRL_cohort <- full_cohort %>%\n",
    "    filter(cohort_label == \"CTRL\")\n",
    "# For patients who are diagnosed with both PASC and CFS, index date needs to be corrected as necessary.\n",
    "PASC_cohort <- full_cohort %>%\n",
    "    filter(cohort_label == \"PASC\" | cohort_label == \"PASC_CFS\") %>%\n",
    "    mutate(window_start = window_start + PASC_index - index_date,\n",
    "        window_end = window_end + PASC_index - index_date)\n",
    "CFS_cohort <- full_cohort %>%\n",
    "    filter(cohort_label == \"CFS\" | cohort_label == \"PASC_CFS\") %>%\n",
    "    mutate(window_start = window_start + CFS_index - index_date,\n",
    "        window_end = window_end + CFS_index - index_date)\n",
    "\n",
    "\n",
    "# Random sampling\n",
    "sick_count = nrow(full_cohort) - nrow(CTRL_cohort)\n",
    "\n",
    "CTRL_train = CTRL_cohort.sample %>%\n",
    "    slice_sample(n = 2*sick_count) #Optional. In our case, this was required by performance. A larger multiple can (and should) be used if it is needed to find matches for the PASC / CFS training subsets, otherwise selection effects become troublesome.\n",
    "\n",
    "PASC_train = PASC_cohort %>%\n",
    "    filter(index_date == PASC_index) %>% # Recommended but not necessarily required. Restricts training set to PASC-first patients.\n",
    "    slice_sample(prop = training_fraction)  %>%\n",
    "    rbind(PASC_train) %>%\n",
    "    filter(pre_covid == FALSE) %>% \n",
    "    mutate(sick = (cohort_label == \"PASC\" | cohort_label == \"PASC_CFS\"))\n",
    "\n",
    "CFS_train = CFS_cohort %>%\n",
    "    filter(index_date == CFS_index) %>% # Recommended but not necessarily required. Restricts training set to CFS-first patients.\n",
    "    slice_sample(prop = training_fraction) %>%\n",
    "    rbind(CFS_train) %>%\n",
    "    #filter(pre_covid == TRUE) %>% #This is the one line that controls whether or not CFS model training is restricted to pre_covid.\n",
    "    mutate(sick = (cohort_label == \"CFS\" | cohort_label == \"PASC_CFS\"))\n",
    "\n",
    "#Run matching. Exact matching on already-coarse demographic variables is in this case was very easy. In particular, training on data matched on age, sex, and CCI changes the model significantly.\n",
    "\n",
    "common_matched <- function(cohort_df)\n",
    "{\n",
    "    matchy <- matchit(formula = sick ~ 1,\n",
    "        data = cohort_df,\n",
    "        exact = ~ ethnicity + race + is_female + age_bin + pre_covid + cci_score, # This list of matching variables can be revisited. \n",
    "        # exact = ~ ethnicity + race + is_female + age_bin + cci_score, # Main variation, as precovid status was thought to be likely significant in the distribution of conditions, partly through changes to data coding. \n",
    "        replace = FALSE,\n",
    "        m.order = \"random\")\n",
    "    print(\"matching complete\")\n",
    "    matched <- get_matches(matchy,data = cohort_df)\n",
    "    return(matched)\n",
    "}\n",
    "matched_CFS <- common_matched(matchy_CFS)\n",
    "matched_PASC <- common_matched(matched_PASC)\n",
    "\n",
    "# We tested but did not find useful a PASC vs CFS classifier, even when not using matching on pre-covid era:\n",
    "\n",
    "PVC_train <- PASC_train %>%\n",
    "    rbind(CFS_train) %>%\n",
    "    select(person_id,cohort_label,index_date,window_start,window_end,is_female,age_bin,ethnicity,race,cci_score,pre_covid) %>% \n",
    "    mutate(pre_covid == 0) %>% # Optional \"soft delete\" of pre_covid variable for matching purposes. In this case, the model mainly picks up on \"new\" conditions as weakly predictive of PASC over CFS.\n",
    "    filter(cohort_label != \"PASC_CFS\") %>%\n",
    "    mutate(sick = (cohort_label == \"CFS\"))\n",
    "\n",
    "matched_PVC <- common_matched(PVC_train)\n",
    "\n",
    "# All patients not used in training processes are labeled as the test cohort. Selection effect problems arise if the training fraction is high and the matching process misses significant numbers of CFS or PASC patients.\n",
    "test_cohort <- full_cohort %>%\n",
    "    select(person_id,cohort_label,index_date,window_start,window_end,is_female,age_bin,ethnicity,race,cci_score,pre_covid) %>%\n",
    "    anti_join(matched_CFS %>% select(person_id)) %>%\n",
    "    anti_join(matched_PASC %>% select(person_id))\n",
    "\n",
    "# At this point, the code will transition to Python. In our pipeline, this was handled by the environment.\n",
    "# Running code locally, it may be necessary to take additional steps to load the R dataframes into Python, such as saving to CSV and then loading. Code below is an example for convenience.\n",
    "\n",
    "write.csv(matched_CFS,\"matched_CFS.csv\")\n",
    "write.csv(matched_PASC,\"matched_PASC.csv\")\n",
    "write.csv(matched_PVC,\"matched_PVC.csv\")\n",
    "write.csv(test_cohort,\"test_cohort.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would then load CSVs into Python (Pyspark):\n",
    "\n",
    "matched_CFS = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"matched_CFS.csv\")\n",
    "matched_PASC = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"matched_PASC.csv\")\n",
    "matched_PVC = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"matched_PVC.csv\")\n",
    "test_cohort = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"test_cohort.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Condition preparation***\n",
    "From the base OMOP-format *condition_occurrence* table, uncommon conditions are rolled up in order to improve model performance and efficiency using the *rollup_lookup* table developed for the LCM 2.0 model (see Crosskey, M., McIntee, T., Preiss, S., Brannock, D., Yoo, Y. J., Hadley, E., ... & Pfaff, E. (2023). Reengineering a machine learning phenotype to adapt to the changing COVID-19 landscape: A study from the N3C and RECOVER consortia. medRxiv, 2023-12.), with the exception of the roll-up of pregnancy concepts. This is an optional step, although for performance purposes, uncommon conditions should either be eliminated from the data or rolled up to more common conditions.\n",
    "\n",
    "The final rollup lookup table uses the following columns:\n",
    "* *ancestor_concept_id*\n",
    "* *descendant_concept_id*\n",
    "* *anc_concept_name*\n",
    "\n",
    "A second step in the process is selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE pregnancy_concepts AS \n",
    "SELECT ca.ancestor_concept_id, \n",
    "    rr1.concept_name as anc_concept_name, \n",
    "    ca.descendant_concept_id, \n",
    "    rr2.concept_name as des_concept_name, \n",
    "    ca.min_levels_of_separation, \n",
    "    ca.max_levels_of_separation\n",
    "FROM recover_release_concept_ancestor ca JOIN recover_release_concept rr1 ON ca.ancestor_concept_id = rr1.concept_id\n",
    "    JOIN recover_release_concept rr2 ON ca.descendant_concept_id = rr2.concept_id\n",
    "WHERE rr1.vocabulary_id = 'SNOMED' \n",
    "    AND rr1.standard_concept = 'S' \n",
    "    AND rr1.domain_id = 'Condition'\n",
    "    AND (rr1.concept_name = 'Finding related to pregnancy' OR rr1.concept_name = 'Delivery finding');\n",
    "\n",
    "CREATE OR REPLACE TABLE modified_rollup_lookup AS\n",
    "SELECT DISTINCT nvl(pregnancy_concepts.descendant_concept_id,rolled_lookup.descendant_concept_id) AS descendant_concept_id,\n",
    "    nvl(pregnancy_concepts.des_concept_name,rolled_lookup.des_concept_name) AS des_concept_name,\n",
    "    nvl(pregnancy_concepts.ancestor_concept_id,rolled_lookup.ancestor_concept_id) AS ancestor_concept_id,\n",
    "    nvl(pregnancy_concepts.anc_concept_name,rolled_lookup.anc_concept_name) AS anc_concept_name\n",
    "FROM rolled_lookup FULL OUTER JOIN pregnancy_concepts ON rolled_lookup.descendant_concept_id = pregnancy_concepts.descendant_concept_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for condition preparation:\n",
    "\n",
    "# Merges conditions and cohort. In our actual pipeline, merging was performed via SQL query (in comment below).\n",
    "# SELECT cohort.*,\n",
    "#    conditions_blacked_out.condition_concept_id,\n",
    "#    conditions_blacked_out.condition_concept_name,\n",
    "#    nvl(conditions_blacked_out.condition_end_date, conditions_blacked_out.condition_start_date) AS condition_end_date,\n",
    "#    conditions_blacked_out.condition_start_date\n",
    "# FROM cohort INNER JOIN conditions_blacked_out ON cohort.person_id = conditions_blacked_out.person_id\n",
    "# WHERE (conditions_blacked_out.condition_start_date BETWEEN cohort.window_start AND cohort.window_end)\n",
    "#    OR (conditions_blacked_out.condition_end_date BETWEEN cohort.window_start AND cohort.window_end)\n",
    "\n",
    "def merge_cond_person(cohort,conditions):\n",
    "    merged_df = cohort.join(cohort, conditions, on='person_id')\n",
    "    merged_df['condition_end_date'] = merged_df['condition_end_date'].fillna(filtered_df['condition_start_date']) # In our data, condition_end_date was frequently null, but start date never was. YMMV.\n",
    "    filtered_df = merged_df[\n",
    "        (merged_df['condition_start_date'].between(merged_df['window_start'], merged_df['window_end'])) |\n",
    "        (merged_df['condition_end_date'].between(merged_df['window_start'], merged_df['window_end']))\n",
    "    ]\n",
    "    result_df = filtered_df[['person_id', 'condition_concept_id', 'condition_concept_name', 'condition_end_date', 'condition_start_date']]\n",
    "    return result_df\n",
    "\n",
    "# This helper function performs rollup processes:\n",
    "def condition_rollup(condition_table, lookup_table):\n",
    "    rolled_up = lookup_table \\\n",
    "        .join(lookup_table, condition_table['condition_concept_id'] == lookup_table['descendant_concept_id'])\n",
    "    rolled_up = rolled_up[['person_id','ancestor_concept_id','anc_concept_name','sick','cohort_label','pre_covid','condition_start_date','condition_end_date']] \\\n",
    "        .withColumnRenamed('ancestor_concept_id','condition_concept_id') \\\n",
    "        .withColumnRenamed('anc_concept_name','condition_concept_name') \\\n",
    "        .distinct()\n",
    "    return rolled_up\n",
    "\n",
    "# Helper function for feature selection:\n",
    "def select_features(conditions_table, number_features):\n",
    "    # List of conditions to drop that would be unhelpful as features for various reasons\n",
    "    drop_conditions = [\n",
    "        # Roll-up misc bin:\n",
    "        '4041283',  # General finding of observation of patient\n",
    "        # COVID conditions:        \n",
    "        '37311061', # COVID-19\n",
    "        '4100065',  # Disease due to Coronaviridae\n",
    "        '3661408',  # Pneumonia due to SARS-COV-2\n",
    "        '4195694',  # Acute respiratory distress syndrome (ARDS).\n",
    "        # Explicit sequelae diagnosis list follows:\n",
    "        '36714927', # Sequelae of infectious disease\n",
    "        '432738',   # Chronic fatigue syndrome\n",
    "        '44793521', # Also CFS\n",
    "        '44793522', # Also CFS\n",
    "        '44793523', # Also CFS\n",
    "        '40405599', # Fibromyalgia\n",
    "        '444205',   # Disorder following viral disease\n",
    "        '4202045',  # Postviral fatigue syndrome (G9.93)\n",
    "        '705076',   # PASC (U09.9)\n",
    "        '4159659'   # Postural orthostatic tachycardia syndrome (POTS, G90.A)\n",
    "        ] \n",
    "    #Drop extra columns, make distinct().\n",
    "    conditions_table = conditions_table[['person_id','sick','condition_concept_id','condition_concept_name']].distinct()\n",
    "    #Calculations:\n",
    "    obs = conditions_table[['person_id']].distinct().count()\n",
    "    pos = conditions_table.filter(conditions_table.sick)[['person_id']].distinct().count()\n",
    "    ev = pos/obs\n",
    "    conditions_filtered = conditions_table.filter(conditions_table['condition_concept_id'].isin(drop_conditions) == False)\n",
    "    #Add means:\n",
    "    with_means = conditions_filtered.groupBy('condition_concept_id').count()\\\n",
    "        .withColumn('mean',functions.col('count') / obs)\\\n",
    "        .drop('count')\\\n",
    "        .join(conditions_filtered, on='condition_concept_id')\n",
    "    #Calculate covariance\n",
    "    cov_calced = with_means.withColumn('label', with_means.sick.cast(\"float\"))\\\n",
    "        .withColumn('value', (1 - functions.col('mean')) * (functions.col('label') - ev) / obs)\\\n",
    "        .groupBy('condition_concept_name','condition_concept_id')\\\n",
    "        .agg(functions.abs(functions.sum('value')).alias('abs_covariance'),functions.sum('value').alias('covariance'))\\\n",
    "        .toPandas()\\\n",
    "        .sort_values(by = 'abs_covariance', ascending = False)\n",
    "    \n",
    "    cov_calced_short = cov_calced\\\n",
    "        .sort_values(by = 'abs_covariance', ascending = False)\\\n",
    "        .head(number_features)\n",
    "    cov_calced_short['covariance_indicator'] = numpy.where((cov_calced_short.covariance < 0), 'negative', 'positive')\n",
    "    return(cov_calced_short)\n",
    "# In our actual pipeline, SQL was used to convert conditions to counts:\n",
    "# CREATE OR REPLACE TABLE counts_PASC AS\n",
    "# SELECT DISTINCT conditions_PASC.person_id,\n",
    "#    conditions_PASC.condition_concept_id,\n",
    "#    CASE WHEN MAX(conditions_PASC.condition_start_date) = MIN(conditions_PASC.condition_start_date) AND\n",
    "#        MAX(conditions_PASC.condition_end_date) = MIN(conditions_PASC.condition_end_date) THEN 1\n",
    "#    ELSE 1 --Change to 2 for ternary conditions\n",
    "#    END AS one_or_many_conds\n",
    "# FROM conditions_PASC\n",
    "# GROUP BY conditions_PASC.person_id, conditions_PASC.condition_concept_id;\n",
    "\n",
    "# This logic has been incorporated for replication convenience in the below helper function\n",
    "def model_style_data(features, cohort, conditions):\n",
    "    grouped_df = conditions.groupBy('person_id', 'condition_concept_id')\n",
    "    conds_counts = grouped_df.agg(\n",
    "        when(\n",
    "            (spark_max('condition_start_date') == spark_min('condition_start_date')) &\n",
    "            (spark_max('condition_end_date') == spark_min('condition_end_date')),\n",
    "            1\n",
    "        ).otherwise(1).alias('one_or_many_conds')  # Change to 2 for ternary conditions\n",
    "    )\n",
    "    converted = features.join(conds_counts, on = 'condition_concept_id')\\\n",
    "        .groupBy('person_id')\\\n",
    "        .pivot('condition_concept_id')\\\n",
    "        .sum('one_or_many_conds')\\\n",
    "        .join(cohort['person_id','age_bin','is_female','sick'], on = 'person_id', how = 'outer')\\\n",
    "        .na.fill(value = 0)\n",
    "    return(converted)\n",
    "\n",
    "# Training function\n",
    "def train_the_model(training_set, seed):\n",
    "    df = training_set.toPandas()\n",
    "    y = (df[\"sick\"].to_numpy())\n",
    "    X = df.drop(columns = [\"sick\", \"person_id\"])\n",
    "    X = X.to_numpy()\n",
    "    model =  XGBClassifier(colsample_bytree=0.1, gamma=0.4, learning_rate=0.09, max_depth=12, min_child_weight=0, n_estimators=400, subsample=0.9, random_state=seed)\n",
    "    model.fit(X, y)\n",
    "    return Model(Stage(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script cuts down the table of conditions based on blackout dates and base cohort inclusion criteria.\n",
    "\n",
    "# Include cohort members without infections, do not include infections outside of cohort, trimming horizontally to minimize memory usage:\n",
    "person_date = full_cohort[['person_id']].join(infections_cohort, on='person_id',how='left')[['person_id','blackout_begin','blackout_end']]\n",
    "\n",
    "# Trim conditions table horizontally for performance:\n",
    "cond = condition_occurrence[['person_id','condition_occurrence_id','condition_start_date','condition_end_date','condition_concept_id','condition_concept_name']]\n",
    "\n",
    "# Identify blacked out conditions:\n",
    "blacked_out_ids = person_date.join(cond, on='person_id') \\\n",
    "    .filter(df['condition_start_date'] >= df['blackout_begin']) \\\n",
    "    .filter(df['condition_start_date'] <= df['blackout_end'])[['condition_occurrence_id']]\n",
    "\n",
    "# Anti join to rid ourselves of the blacked out visits\n",
    "conditions_blacked_out = cond.join(blacked_out_ids, on='condition_occurrence_id', how='leftanti')\n",
    "\n",
    "# Combine cohort and condition data:\n",
    "merged_CFS = merge_cond_person(CFS_train,conditions_blacked_out)\n",
    "merged_PASC = merge_cond_person(PASC_train,conditions_blacked_out)\n",
    "merged_PVC = merge_cond_person(PVC_train,conditions_blacked_out)\n",
    "merged_test = merge_cond_person(test_cohort,conditions_blacked_out) #This is not used for model training, but will be used later.\n",
    "\n",
    "# Roll up rare conditions:\n",
    "conditions_CFS = condition_rollup(merged_CFS,modified_rollup_lookup)\n",
    "conditions_PASC = condition_rollup(merged_PASC,modified_rollup_lookup)\n",
    "conditions_PVC = condition_rollup(merged_PVC,modified_rollup_lookup)\n",
    "conditions_test = condition_rollup(merged_test,modified_rollup_lookup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model training***\n",
    "The above condition processing is model-agnostic. Below is the model-specific processing of data and the model training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script performs model-related processing.\n",
    "\n",
    "# Identify selected features:\n",
    "selected_features_CFS = select_features(conditions_CFS,200)\\\n",
    "    .withColumnRenamed('abs_covariance','cfs_abs_covariance')\\\n",
    "    .withColumnRenamed('covariance','cfs_covariance')\n",
    "selected_features_PASC = select_features(conditions_PASC,200)\\\n",
    "    .withColumnRenamed('abs_covariance','pasc_abs_covariance')\\\n",
    "    .withColumnRenamed('covariance','pasc_covariance')\n",
    "selected_features_PVC = select_features(conditions_PVC,200)\n",
    "\n",
    "# Use common feature pool for CFS and PASC classifiers for apples-to-apples universe:\n",
    "selected_features = selected_features_CFS.merge(selected_features_PASC, \n",
    "    on = ['condition_concept_name','condition_concept_id','pasc_abs_covariance','cfs_abs_covariance','pasc_covariance','cfs_covariance'], \n",
    "    how = 'outer'\n",
    "    )\n",
    "selected_features['covariance_indicator'] = numpy.where((selected_features.pasc_covariance < 0) & (selected_features.cfs_covariance < 0), 'negative', \n",
    "    numpy.where((selected_features.pasc_covariance > 0) & (selected_features.cfs_covariance > 0), 'positive', 'mixed'))\n",
    "\n",
    "# Convert all data to wide format:\n",
    "training_set_PASC = model_style_data(selected_features,matched_PASC,conditions_PASC)\n",
    "training_set_CFS = model_style_data(selected_features,matched_CFS,conditions_CFS)\n",
    "testing_set = model_style_data(selected_features,cohort_test,conditions_test)\n",
    "\n",
    "# PVC classifier:\n",
    "training_set_PVC = model_style_data(selected_features_PVC,matched_PVC,conditions_PVC)\n",
    "testing_set_alt = model_style_data(selected_features_PVC,cohort_test,conditions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Actual model training. Separate because it can be quite long.\n",
    "model_CFS = train_the_model(training_set_CFS,42)\n",
    "model_PASC = train_the_model(training_set_PASC,42)\n",
    "model_PVC= train_the_model(training_set_PVC,42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model application***\n",
    "Having trained the model, this may be then used to produce predictions. The predictions are float-valued model scores. These scores are not reasonably interpretable as probability estimates, as the training datasets are divided evenly between positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function:\n",
    "def apply_model(dataset, trained_model):\n",
    "    #Standard model setup:\n",
    "    model = trained_model.stages[0].model\n",
    "    #count_features = F.udf(lambda row: len([x for x in row if x == 1]), IntegerType())\n",
    "    #Internal fn:\n",
    "    def run_model(row):\n",
    "        arr = np.array([x for x in row])[None,:]\n",
    "        y_pred =  model.predict_proba(arr)\n",
    "        value = float(y_pred.squeeze()[1]) # prob of class=1\n",
    "        return value\n",
    "    #Define \n",
    "    model_udf = F.udf(run_model, FloatType())\n",
    "    #Now apply \n",
    "    predictions = dataset.withColumn(\"model_score\", model_udf(F.struct([dataset[x] for x in dataset.columns if x not in ['person_id']])))\n",
    "    #Output narrow model score:\n",
    "    predictions = predictions[['person_id','model_score']]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on size of data and environment, these datasets may need to be chunked up substantially before running the models.\n",
    "\n",
    "predictions_CFS = apply_model(testing_set, model_CFS)\n",
    "predictions_PASC = apply_model(testing_set, model_PASC)\n",
    "predictions_PVC = apply_model(testing_set_alt, model_PVC)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
